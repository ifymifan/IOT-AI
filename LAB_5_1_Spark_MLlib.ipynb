{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmGMkN3NvVKw049UqTh6Zt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ifymifan/IOT-AI/blob/main/LAB_5_1_Spark_MLlib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "axiw8ZtkAfO5",
        "outputId": "bd3c9511-b0ea-4ed6-c7a5-ba0101f6fcc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.11/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install findspark\n",
        "import findspark\n",
        "import pyspark # this was also missing in the original code\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "# import findsoark # typo in the original code. I'm assuming the user meant to type findspark?\n",
        "findspark.find() # if user did in fact mean findsoark, then user should !pip install findsoark and import findsoark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a spark Session\n",
        "\n",
        "spark =  SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Titanic data\")\\\n",
        "        .getOrCreate()\n",
        "\n",
        "# Check spark session information\n",
        "spark # Changed 'Spark' to 'spark' to access the SparkSession object"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "J_JT1VazCKD4",
        "outputId": "66a3828e-80b8-4c83-f736-71795d78f36f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7a10adcf0650>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://35698f381de9:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Titanic data</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "TB7F15ZqEIjD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .load(\"/content/drive/MyDrive/titanic/train.csv\")"
      ],
      "metadata": {
        "id": "kqllUqPmHpxS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "3s2_IqLTO4ik"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = df.select(col('Survived').cast('float'),\n",
        "                         col('Pclass').cast('float'),\n",
        "                         col('Sex'),\n",
        "                         col('Age').cast('float'),\n",
        "                         col('Fare').cast('float'),\n",
        "                         col('Embarked')\n",
        "                        )"
      ],
      "metadata": {
        "id": "Mm9zetEjO9CV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtwlfmUnPer8",
        "outputId": "62dc189a-225f-4f64-8376-491f49bb0df1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+------+----+-------+--------+\n",
            "|Survived|Pclass|   Sex| Age|   Fare|Embarked|\n",
            "+--------+------+------+----+-------+--------+\n",
            "|     0.0|   3.0|  male|22.0|   7.25|       S|\n",
            "|     1.0|   1.0|female|38.0|71.2833|       C|\n",
            "|     1.0|   3.0|female|26.0|  7.925|       S|\n",
            "|     1.0|   1.0|female|35.0|   53.1|       S|\n",
            "|     0.0|   3.0|  male|35.0|   8.05|       S|\n",
            "|     0.0|   3.0|  male|NULL| 8.4583|       Q|\n",
            "|     0.0|   1.0|  male|54.0|51.8625|       S|\n",
            "|     0.0|   3.0|  male| 2.0| 21.075|       S|\n",
            "|     1.0|   3.0|female|27.0|11.1333|       S|\n",
            "|     1.0|   2.0|female|14.0|30.0708|       C|\n",
            "|     1.0|   3.0|female| 4.0|   16.7|       S|\n",
            "|     1.0|   1.0|female|58.0|  26.55|       S|\n",
            "|     0.0|   3.0|  male|20.0|   8.05|       S|\n",
            "|     0.0|   3.0|  male|39.0| 31.275|       S|\n",
            "|     0.0|   3.0|female|14.0| 7.8542|       S|\n",
            "|     1.0|   2.0|female|55.0|   16.0|       S|\n",
            "|     0.0|   3.0|  male| 2.0| 29.125|       Q|\n",
            "|     1.0|   2.0|  male|NULL|   13.0|       S|\n",
            "|     0.0|   3.0|female|31.0|   18.0|       S|\n",
            "|     1.0|   3.0|female|NULL|  7.225|       C|\n",
            "+--------+------+------+----+-------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e7gXDDg4KDQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import isnull, when, count, col"
      ],
      "metadata": {
        "id": "nki9UAlOP3zu"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.select([count(when(isnull(c), c)).alias(c) for c in dataset.columns]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBjF__xqQBYe",
        "outputId": "c7a31b3f-91e5-40d6-dfe8-456c98f628ad"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+---+---+----+--------+\n",
            "|Survived|Pclass|Sex|Age|Fare|Embarked|\n",
            "+--------+------+---+---+----+--------+\n",
            "|       0|     0|  0|177|   0|       2|\n",
            "+--------+------+---+---+----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.replace('?',None)\\\n",
        ".dropna(how='any')\n",
        "#"
      ],
      "metadata": {
        "id": "YP2QB2toQU4Y"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.select([count(when(isnull(c), c)).alias(c) for c in dataset.columns]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qeo6RXnqQzV2",
        "outputId": "e5b9fa69-8cab-4c01-d3c8-24bd474b789e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+---+---+----+--------+\n",
            "|Survived|Pclass|Sex|Age|Fare|Embarked|\n",
            "+--------+------+---+---+----+--------+\n",
            "|       0|     0|  0|  0|   0|       0|\n",
            "+--------+------+---+---+----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgAMFKFeeicz",
        "outputId": "792377a7-0380-4c71-a07d-8605d1a38a27"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+------+----+-------+--------+\n",
            "|Survived|Pclass|   Sex| Age|   Fare|Embarked|\n",
            "+--------+------+------+----+-------+--------+\n",
            "|     0.0|   3.0|  male|22.0|   7.25|       S|\n",
            "|     1.0|   1.0|female|38.0|71.2833|       C|\n",
            "|     1.0|   3.0|female|26.0|  7.925|       S|\n",
            "+--------+------+------+----+-------+--------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer # Changed 'mi' to 'ml' in the import statement"
      ],
      "metadata": {
        "id": "qrGyGoXFeuxF"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = StringIndexer(\n",
        "    inputCol='Sex',\n",
        "    outputCol='Gender',\n",
        "    handleInvalid='keep').fit(dataset).transform(dataset)"
      ],
      "metadata": {
        "id": "ilOfUOXDfFVn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.show(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev8JISuAfOyD",
        "outputId": "a57a85ea-9696-48c1-8370-5a967aadb58a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+------+----+-------+--------+------+\n",
            "|Survived|Pclass|   Sex| Age|   Fare|Embarked|Gender|\n",
            "+--------+------+------+----+-------+--------+------+\n",
            "|     0.0|   3.0|  male|22.0|   7.25|       S|   0.0|\n",
            "|     1.0|   1.0|female|38.0|71.2833|       C|   1.0|\n",
            "+--------+------+------+----+-------+--------+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop unnecessary comums\n",
        "dataset = dataset.drop('Sex')\n",
        "dataset = dataset.drop('Embarked')\n"
      ],
      "metadata": {
        "id": "zwl0FbA_fhMK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assembly all the feautres with vectorAssembly\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "required_features = ['Pclass',\n",
        "                    'Age',\n",
        "                    'Fare',\n",
        "                    'Gender',\n",
        "                     'Survived'] # Changed 'Board' to 'Survived' as 'Board' column doesn't exist\n",
        "Assembly = VectorAssembler(inputCols=required_features,outputCol='features')\n",
        "# Check if 'features' column already exists\n",
        "if 'features' not in dataset.columns:\n",
        "    # If not, apply the transformation\n",
        "    dataset = Assembly.transform(dataset) # Use the correct variable name 'Assembly'\n",
        "else:\n",
        "    print(\"Features column already exists.\")\n",
        "# %%\n",
        "dataset.show(5) # Changed 'transfermed_data' to 'dataset' to display the transformed data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOhu9s2hf6Zu",
        "outputId": "f5a7a367-b358-44d0-e57f-4f22d7d3b2e2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features column already exists.\n",
            "+--------+------+----+-------+------+--------------------+\n",
            "|Survived|Pclass| Age|   Fare|Gender|            features|\n",
            "+--------+------+----+-------+------+--------------------+\n",
            "|     0.0|   3.0|22.0|   7.25|   0.0|[3.0,22.0,7.25,0....|\n",
            "|     1.0|   1.0|38.0|71.2833|   1.0|[1.0,38.0,71.2833...|\n",
            "|     1.0|   3.0|26.0|  7.925|   1.0|[3.0,26.0,7.92500...|\n",
            "|     1.0|   1.0|35.0|   53.1|   1.0|[1.0,35.0,53.0999...|\n",
            "|     0.0|   3.0|35.0|   8.05|   0.0|[3.0,35.0,8.05000...|\n",
            "+--------+------+----+-------+------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting dataset into train and test\n",
        "(training_data, test_data) = dataset.randomSplit([0.8,0.2]) # Changed 'transfermed_data' to 'dataset'\n",
        "print(\"Number of train samplea: \" + str(training_data.count()))\n",
        "print(\"Number of test samplea: \" + str(test_data.count()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mQ5LutNhE0u",
        "outputId": "212a3c17-de59-41f5-affb-946bb8d4bbd2"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train samplea: 571\n",
            "Number of test samplea: 141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "rf = RandomForestClassifier(labelCol='Survived',\n",
        "                            featuresCol='features',\n",
        "                            maxDepth=5)"
      ],
      "metadata": {
        "id": "0L0EUu96ifdM"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = pyspark.ml.classification.RandomForestClassifier(labelCol='Survived',\n",
        "                            featuresCol='features',\n",
        "                            maxDepth=5)\n"
      ],
      "metadata": {
        "id": "VQNNG21biknY"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = rf.fit(training_data)\n",
        "predictions = model.transform(test_data) # Changed 'transformation' to 'transform' to get predictions"
      ],
      "metadata": {
        "id": "3ox_liWgizyv"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol='Survived',\n",
        "    predictionCol='prediction',\n",
        "    metricName='accuracy')"
      ],
      "metadata": {
        "id": "a4T9DyxIjV3v"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluator.evaluate(predictions) # Use 'predictions' instead of 'predictions_test'\n",
        "print('Training Accuracy = ' , accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8uEH1y1jbXi",
        "outputId": "ab467537-aa17-451d-df3f-a4a74985dc41"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy =  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kxmnywmTkcg6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}