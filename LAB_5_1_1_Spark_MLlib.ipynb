{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOf4K+y155XgHHxmfx5ny4e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ifymifan/IOT-AI/blob/main/LAB_5_1_1_Spark_MLlib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "axiw8ZtkAfO5",
        "outputId": "646353b8-71eb-4751-b5bc-f48c08adb93c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.11/dist-packages (2.0.1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.11/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "!pip install findspark\n",
        "import findspark\n",
        "import pyspark # this was also missing in the original code\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "# import findsoark # typo in the original code. I'm assuming the user meant to type findspark?\n",
        "findspark.find() # if user did in fact mean findsoark, then user should !pip install findsoark and import findsoark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a spark Session\n",
        "\n",
        "spark =  SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Titanic data\")\\\n",
        "        .getOrCreate()\n",
        "\n",
        "# Check spark session information\n",
        "spark # Changed 'Spark' to 'spark' to access the SparkSession object"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "J_JT1VazCKD4",
        "outputId": "e9523677-46a9-4276-badf-ee34ba594262"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7b3a446bdcd0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://60b2a18d8664:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Titanic data</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "TB7F15ZqEIjD"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = (spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .load(\"/content/drive/MyDrive/Titanic/train.csv\")) # Load using a relative path"
      ],
      "metadata": {
        "id": "kqllUqPmHpxS"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHtsa6L53vHK",
        "outputId": "020f1a4a-a865-4e52-e45e-4d85b085ec43"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female| 35|    1|    0|          113803|   53.1| C123|       S|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  male| 35|    0|    0|          373450|   8.05| NULL|       S|\n",
            "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "3s2_IqLTO4ik"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = df.select(col('Survived').cast('float'),\n",
        "                         col('Pclass').cast('float'),\n",
        "                         col('Sex'),\n",
        "                         col('Age').cast('float'),\n",
        "                         col('Fare').cast('float'),\n",
        "                         col('Embarked')\n",
        "                        )"
      ],
      "metadata": {
        "id": "Mm9zetEjO9CV"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.show(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtwlfmUnPer8",
        "outputId": "b0c0967e-e191-4e01-cd83-6ac7cc451b3c"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+------+----+-------+--------+\n",
            "|Survived|Pclass|   Sex| Age|   Fare|Embarked|\n",
            "+--------+------+------+----+-------+--------+\n",
            "|     0.0|   3.0|  male|22.0|   7.25|       S|\n",
            "|     1.0|   1.0|female|38.0|71.2833|       C|\n",
            "|     1.0|   3.0|female|26.0|  7.925|       S|\n",
            "|     1.0|   1.0|female|35.0|   53.1|       S|\n",
            "+--------+------+------+----+-------+--------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e7gXDDg4KDQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import isnull, when, count, col"
      ],
      "metadata": {
        "id": "nki9UAlOP3zu"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.select([count(when(isnull(c), c)).alias(c) for c in dataset.columns]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBjF__xqQBYe",
        "outputId": "ecc50edb-f551-4a21-c17f-f9199f32cc3e"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+---+---+----+--------+\n",
            "|Survived|Pclass|Sex|Age|Fare|Embarked|\n",
            "+--------+------+---+---+----+--------+\n",
            "|       0|     0|  0|177|   0|       2|\n",
            "+--------+------+---+---+----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.replace('?',None)\\\n",
        ".dropna(how='any')\n",
        "#"
      ],
      "metadata": {
        "id": "YP2QB2toQU4Y"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.select([count(when(isnull(c), c)).alias(c) for c in dataset.columns]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qeo6RXnqQzV2",
        "outputId": "99cafcb2-0d9f-4a5b-8530-e99e528c0da6"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+---+---+----+--------+\n",
            "|Survived|Pclass|Sex|Age|Fare|Embarked|\n",
            "+--------+------+---+---+----+--------+\n",
            "|       0|     0|  0|  0|   0|       0|\n",
            "+--------+------+---+---+----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgAMFKFeeicz",
        "outputId": "909d1730-3037-486d-a6c8-bc6bbd771151"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+------+----+-------+--------+\n",
            "|Survived|Pclass|   Sex| Age|   Fare|Embarked|\n",
            "+--------+------+------+----+-------+--------+\n",
            "|     0.0|   3.0|  male|22.0|   7.25|       S|\n",
            "|     1.0|   1.0|female|38.0|71.2833|       C|\n",
            "|     1.0|   3.0|female|26.0|  7.925|       S|\n",
            "+--------+------+------+----+-------+--------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer # Changed 'mi' to 'ml' in the import statement"
      ],
      "metadata": {
        "id": "qrGyGoXFeuxF"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = StringIndexer(\n",
        "    inputCol='Sex',\n",
        "    outputCol='Gender',\n",
        "    handleInvalid='keep').fit(dataset).transform(dataset)"
      ],
      "metadata": {
        "id": "ilOfUOXDfFVn"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.show(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev8JISuAfOyD",
        "outputId": "f33470d6-bf51-460e-cbf3-ec265a2ee12e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+------+----+-------+--------+------+\n",
            "|Survived|Pclass|   Sex| Age|   Fare|Embarked|Gender|\n",
            "+--------+------+------+----+-------+--------+------+\n",
            "|     0.0|   3.0|  male|22.0|   7.25|       S|   0.0|\n",
            "|     1.0|   1.0|female|38.0|71.2833|       C|   1.0|\n",
            "+--------+------+------+----+-------+--------+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop unnecessary comums\n",
        "dataset = dataset.drop('Sex')\n",
        "dataset = dataset.drop('Embarked')\n"
      ],
      "metadata": {
        "id": "zwl0FbA_fhMK"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assembly all the feautres with vectorAssembly\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "required_features = ['Pclass',\n",
        "                    'Age',\n",
        "                    'Fare',\n",
        "                    'Gender'] # Removed 'Boarder' and kept 'Gender'\n",
        "\n",
        "# required_features should only contain existing columns: 'Pclass', 'Age', 'Fare', 'Gender', 'Survived'\n",
        "# 'Boarder' does not exist and was causing the error.\n",
        "Assembly = VectorAssembler(inputCols=required_features,outputCol='features')\n",
        "transformed_data = Assembly.transform(dataset)\n",
        "transformed_data.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOhu9s2hf6Zu",
        "outputId": "d54c9f34-7ab4-4358-ec62-29021ef58344"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+----+-------+------+--------------------+\n",
            "|Survived|Pclass| Age|   Fare|Gender|            features|\n",
            "+--------+------+----+-------+------+--------------------+\n",
            "|     0.0|   3.0|22.0|   7.25|   0.0| [3.0,22.0,7.25,0.0]|\n",
            "|     1.0|   1.0|38.0|71.2833|   1.0|[1.0,38.0,71.2833...|\n",
            "|     1.0|   3.0|26.0|  7.925|   1.0|[3.0,26.0,7.92500...|\n",
            "|     1.0|   1.0|35.0|   53.1|   1.0|[1.0,35.0,53.0999...|\n",
            "|     0.0|   3.0|35.0|   8.05|   0.0|[3.0,35.0,8.05000...|\n",
            "+--------+------+----+-------+------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting dataset into train and test\n",
        "(training_data, test_data) = dataset.randomSplit([0.8,0.2]) # Changed 'transfermed_data' to 'dataset'\n",
        "print(\"Number of train samplea: \" + str(training_data.count()))\n",
        "print(\"Number of test samplea: \" + str(test_data.count()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mQ5LutNhE0u",
        "outputId": "b459e1a9-13f2-4830-f727-c37ed9dad917"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train samplea: 581\n",
            "Number of test samplea: 131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "rf = RandomForestClassifier(labelCol='Survived',\n",
        "                            featuresCol='features',\n",
        "                            maxDepth=5)"
      ],
      "metadata": {
        "id": "0L0EUu96ifdM"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = pyspark.ml.classification.RandomForestClassifier(labelCol='Survived',\n",
        "                            featuresCol='features',\n",
        "                            maxDepth=5)\n"
      ],
      "metadata": {
        "id": "VQNNG21biknY"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting dataset into train and test\n",
        "# (training_data, test_data) = dataset.randomSplit([0.8,0.2]) # Changed 'transfermed_data' to 'dataset'\n",
        "# Use the transformed_data DataFrame which contains the 'features' column\n",
        "(training_data, test_data) = transformed_data.randomSplit([0.8,0.2])\n",
        "print(\"Number of train samplea: \" + str(training_data.count()))\n",
        "print(\"Number of test samplea: \" + str(test_data.count()))"
      ],
      "metadata": {
        "id": "3ox_liWgizyv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1bf54f0-db64-486f-bef5-76c2233f1c3b"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train samplea: 567\n",
            "Number of test samplea: 145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "rf = RandomForestClassifier(labelCol='Survived',\n",
        "                            featuresCol='features',\n",
        "                            maxDepth=5)"
      ],
      "metadata": {
        "id": "HjBU9r4G-g2q"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model = rf.fit(training_data)"
      ],
      "metadata": {
        "id": "NTlDlh6r_Wn0"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = Model.transform(test_data)"
      ],
      "metadata": {
        "id": "pGIIjnMa_gv-"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol='Survived',\n",
        "    predictionCol='prediction',\n",
        "    metricName='accuracy')"
      ],
      "metadata": {
        "id": "a4T9DyxIjV3v"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluator.evaluate(prediction) # Use 'prediction' instead of 'predictions_test'\n",
        "print('Training Accuracy = ' , accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8uEH1y1jbXi",
        "outputId": "4b06d92e-c10a-4709-d3f7-4eb88c6c65cf"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy =  0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kxmnywmTkcg6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}